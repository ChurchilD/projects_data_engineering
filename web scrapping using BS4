from bs4 import BeautifulSoup as soup
import requests 
import pandas as pd 
import re
import datetime
# Identify the target website's address, i.e., URL
books_url = 'https://books.toscrape.com/index.html'
base_url_for_category = 'https://books.toscrape.com/'
base_url_for_book = 'https://books.toscrape.com/catalogue'
# Create a response object to get the web page's HTML content
get_url = requests.get(books_url)
# Create a beautiful soup object to parse HTML text with the help of the html.parser
books_soup = soup(get_url.text, 'html.parser')
# Check website's response
##print(get_url)
# Get some intuition by printing out HTML
# This step is not required to build a web scraper
##print(get_url.text)
##print(books_soup)
# Use prettify() method to make the HTML be nicely formatted
##print(books_soup.prettify())
categories = books_soup.find('ul', {'class': 'nav nav-list'}).find('li').find('ul').find_all('li')
##print (categories)
# Loop through categories
for category in categories: 
# Get category name by extracting the text part of <a> element
# Strip the spaces before and after the name
    category_name = category.find('a').text.strip()
# Get the URL, which leads to the products list page under the category
    category_url_relative = category.find('a').get('href')
# Complete category's URL by adding the base URL
    category_url = base_url_for_category + category_url_relative
    print(f"{category_name}'s URL is: {category_url}")
# Get the date of scraping
scraping_date = datetime.date.today()
# Create a dictionary to convert words to digits
# We will use it when fetching rating
w_to_d = {'One': 1,
'Two': 2,
'Three': 3,
'Four': 4,
'Five': 5
}
# Create a list to store all the extracted items
books_all = []

def find_categories(soup):# Find all the categories

    categories = books_soup.find('ul', {'class': 'nav nav-list'}).find('li').find('ul').find_all('li')

    return categories

def fetch_all_books(soup):
    # Fetch all the books information
    # Return books_all, a list of dictionary that contains all the extracted data
    
    # Find all the categories by running find_categories() function
    categories = find_categories(soup)
    # Loop through categories
    for category in categories:
        # Fetch product by category
        # Within the fetch_books_by_category function, we will scrape products page by page        
        category_name = category.find('a').text.strip()
        fetch_books_by_category(category_name, category)
    ##print (books_all)    
    return books_all   


# Identify the target website's address
web_page_url = 'https://www.omdena.com'
# Create a response object to get the web page's HTML content
get_url = requests.get(web_page_url)
# Create a beautiful soup object to parse HTML text with the help of the html.parser
soup = soup(get_url.text, 'html.parser')

def fetch_books_by_category(category_name, category):
    # Fetch books by category
    # Scrape all the books listed on one page
    # Go to next page if current page is not the last page
    # Break the loop at the last page

    # Get category URL, i.e., the link to the first page of books under the category
    books_page_url = base_url_for_category + category.find('a').get('href')
    # Scape books page by page only when the next page is available
    while True:
    # Retrieve the products list page's HTML
        get_current_page = requests.get(books_page_url)
    # Create a beautiful soup object for the current page
        current_page_soup = soup(get_current_page.text, 'html.parser')
    # Run fetch_current_page_books function to get all the products listed on the current page
        fetch_current_page_books(category_name, current_page_soup)
    # Search for the next page's URL
    # Get the next page's URL if the current page is not the last page
        try:
            find_next_page_url = current_page_soup.find('li', {'class':'next'}).find('a').get('href') 
        # Find the index of the last '/'
            index = books_page_url.rfind('/')
        # Skip the string after the last '/' and add the next page url
            books_page_url = books_page_url[:index+1].strip() + find_next_page_url 
        except:
            break

def fetch_current_page_books(category_name, current_page_soup):
# Fetch all the books listed on the current page
# Build a dictionary to store extracted data 
# Append book information to the books_all list

# Find all products listed on the current page
# Here, we don’t need to identify the class name of <li> (Do you know why?)
    current_page_books = current_page_soup.find('ol', {'class':'row'}).find_all('li')

# Loop through the products 
    for book in current_page_books: 
# Extract book info of interest

# Get book title
# Replace get('title') with text to see what will happen
        title = book.find('h3').find('a').get('title').strip()

# Get book price
        price = book.find('p', {'class':'price_color'}).text.strip()

# Get in stock info
        instock = book.find('p', {'class': 'instock availability'}).text.strip()

# Get rating
# We will get a list, ['star-rating', 'Two'], by using get('class') only, so here, we slice the list to extract rating only
        rating_in_words = book.find('p').get('class')[1]
        rating = w_to_d[rating_in_words]

# Get link 
        link = book.find('h3').find('a').get('href').strip()
        link = base_url_for_book + link.replace('../../..', '')

# Get more info about a book by running fetch_more_info function
        product_info = fetch_more_info(link)

# Create a book dictionary to store the book’s info
        book = {
        'scraping_date': scraping_date, 
        'book_title': title, 
        'category': category_name, 
        'price': price,
        'rating': rating,
        'instock': instock,
        # Suppose we’re only interested in availability and UPC only
        'availability': product_info['Availability'],
        'UPC': product_info['UPC'],
        'link':link 
        }
# Append book dictionary to books_all list
        print(book)
        books_all.append(book)

def fetch_more_info(link):
# Go to the single product page to get more info 
# Get url of the web page
    get_url = requests.get(link)
# Create a beautiful soup object for the book
    book_soup = soup(get_url.text, 'html.parser')

# Find the product information table
    book_table = book_soup.find('table',{'class':'table table-striped'}).find_all('tr')
# Build a dictionary to store the information in the table
    product_info = {}
# Loop through the table 
    for info in book_table:
# Use header cell as key
        key = info.find('th').text.strip()
# Use cell as value
        value = info.find('td').text.strip() 
        product_info[key] = value

# Extract number from availability using Regular Expressions
        text = product_info['Availability']
# reassign the number to availability
        product_info['Availability'] = re.findall(r'(d+)', text)[0]

    return product_info
    
def output(books_list):
    # Convert the list with scraped data to a data frame, drop the duplicates, and save the output as a csv file

    # Convert the list to a data frame, drop the duplicates
    books_df = pd.DataFrame(books_list).drop_duplicates()
    print(f'There are totally {len(books_df)} books.')
    # Save the output as a csv file
    books_df.to_csv(f'books_scraper_{scraping_date}.csv', index = False)

if __name__ == '__main__':
    output(books_all)
